{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMjtOesTP3On9jLYR8Ml6i9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DiegoLangreo7/EII-RI/blob/main/Ejercicio5_RI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ej. 5. Comparación con búsqueda semántica**"
      ],
      "metadata": {
        "id": "vIt08lptuTEH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este nuevo ejercicio, se nos pide que comparemos el rendimiento de la mejor configuración de BM25 con expansión de consultas del cuarto ejercicio (es decir, la que obtuvimos de turnar distintos valores de n con m) contra un sistema de recuperación semántico que emplee una base de datos vectorial y un modelo pre-entrenado de word embeddings—puedes utilizar el del notebook visto en los laboratorios, ademas de realizar un análisis comparativo detallado, discutiendo las fortalezas y debilidades de cada enfoque para diferentes tipos de consulta."
      ],
      "metadata": {
        "id": "P0Ai1l91uX1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para este ejercicio nos apoyaremos en el tercer guion de practicas, el cual trata este tema."
      ],
      "metadata": {
        "id": "Ou_L7Ax_uqo1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lo primero de todo que haremos sera descargarnos los paquetes para utilizar ChromaDB, e importarlos, que es lo usado en practicas y lo que uso de referencia:"
      ],
      "metadata": {
        "id": "dJLSCCZgvKYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb\n",
        "!pip install sentence_transformers\n",
        "\n",
        "import chromadb\n",
        "import json\n",
        "from chromadb.utils import embedding_functions\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "1hupBGumvVSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego nos descargamos nuevamente nuestra coleccion Trec-Covid y posteriormente, lo descomprimimos y parseamos lisa-corpus.json:"
      ],
      "metadata": {
        "id": "zH1FSvBWvc0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargamos la colección y las consultas y las query\n",
        "!gdown 19pzNFYIch8rj9d3kyq-171V8vRa_wLBC\n",
        "!unzip -o trec-covid-RI.zip\n",
        "\n",
        "# Parseamos la colección\n",
        "with open(\"corpus.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    corpus_content = []\n",
        "    for line in f:\n",
        "        corpus_content.append(json.loads(line))"
      ],
      "metadata": {
        "id": "OUexDGEMvnlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para crear la colección, necesitamos instanciar un cliente, lo cual haremos como en practicas:"
      ],
      "metadata": {
        "id": "9iFd8WPSv6_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the sentence transformer model\n",
        "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "# Create a persistent ChromaDB client\n",
        "client = chromadb.PersistentClient(path=\"./chromadb-storage/\")\n",
        "\n",
        "# We create the collection, please note how we are providing the embedding\n",
        "# pre-trained model (this is a multilingual model) and we specify the\n",
        "# distance metric to find the nearest neighbors\n",
        "collection = client.create_collection(\n",
        "  name=\"TREC-COVID\",\n",
        "  embedding_function=embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"),\n",
        "  metadata={\"hnsw:space\": \"cosine\"}\n",
        ")"
      ],
      "metadata": {
        "id": "9Z3TtS1Zv_X2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora preparamos los documentos:"
      ],
      "metadata": {
        "id": "2HUCKqZ2wsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chromadb_documents = []\n",
        "chromadb_doc_ids = []\n",
        "\n",
        "for document in corpus_content:\n",
        "  doc_id = str(document[\"_id\"])\n",
        "  title = document[\"title\"].lower()\n",
        "  content = document[\"text\"].lower()\n",
        "\n",
        "  chromadb_doc_ids.append(doc_id)\n",
        "  chromadb_documents.append(f\"{title} {content}\")\n",
        "\n",
        "\n",
        "# Con esto crearemos los embeddings:\n",
        "chromadb_embeddings = model.encode(chromadb_documents, batch_size=100, show_progress_bar=True, device='cuda')"
      ],
      "metadata": {
        "id": "ayUjYIoswuD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora que tenemos los embeddings de los documentos, podemos comenzar a añadir documentos, identificadores y embeddings a la colección en ChromaDB:"
      ],
      "metadata": {
        "id": "HDpGL4tXw3S5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# En primer lugar, definimos una función para generar lotes:\n",
        "def get_batches(lista, chunk_size=100):\n",
        "    return [lista[i:i + chunk_size] for i in range(0, len(lista), chunk_size)]"
      ],
      "metadata": {
        "id": "HVx8HqVeyQGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A continuación, utilizamos la función anterior para ir creando la colección por lotes:\n",
        "document_batches = get_batches(chromadb_documents)\n",
        "ids_batches = get_batches(chromadb_doc_ids)\n",
        "embedding_batches = get_batches(chromadb_embeddings)\n",
        "\n",
        "for i in range(len(document_batches)):\n",
        "  documents = document_batches[i]\n",
        "  doc_ids = ids_batches[i]\n",
        "  embeddings = embedding_batches[i]\n",
        "\n",
        "  collection.add(\n",
        "    documents=documents,\n",
        "    ids=doc_ids,\n",
        "    embeddings=embeddings\n",
        "  )"
      ],
      "metadata": {
        "id": "eN171h_NyW9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez tenemos la coleccion creada, vamos a proseguir a buscar con chromaDB:\n"
      ],
      "metadata": {
        "id": "3efvs0aiyncB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un cliente persistente de ChromaDB\n",
        "client = chromadb.PersistentClient(path=\"./chromadb-storage/\")\n",
        "\n",
        "# Obtener las colecciones disponibles en ChromaDB\n",
        "existing_collections = client.list_collections()\n",
        "\n",
        "collection_name = \"TREC-COVID\"\n",
        "\n",
        "# Inicializar el modelo transformador de oraciones\n",
        "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "# Verificar si la colección existe\n",
        "if collection_name in [col.name for col in existing_collections]:\n",
        "    # Tiene poco sentido que sea necesario especificar qué función de embeddings usa la\n",
        "    # colección *pero* si no se informa explícitamente,\n",
        "    # Chroma utilizará la función de embeddings predeterminada y será como\n",
        "    # comparar peras con manzanas...\n",
        "    collection = client.get_collection(\n",
        "        collection_name,\n",
        "        embedding_function=embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "    )\n",
        "\n",
        "    existing_ids = collection.get()[\"ids\"]\n",
        "    print(f\"La colección {collection_name} contiene {len(existing_ids)} documentos\")\n",
        "else:\n",
        "    print(f\"¡{collection_name} no existe! Es necesario crearla.\")\n"
      ],
      "metadata": {
        "id": "khEqPPzOyshF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora es necesario cargar las consultas, para ello reutilizaremos codigo de anteriores ejercicios:"
      ],
      "metadata": {
        "id": "qrQeFze2zCsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Queries cargadas\n",
        "queries = []\n",
        "\n",
        "with open(\"queries.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        queries.append(json.loads(line))"
      ],
      "metadata": {
        "id": "ZJ7DSV3dzHsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a crear una función para enviar las consultas y obtener los resultados asociados a ellas, para\n",
        "posteriormente calcular el rendimiento (precisión, exhaustividad y F1-score)."
      ],
      "metadata": {
        "id": "Y59ZWjlc0VJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para cargar consultas desde un archivo JSON para enviarlas a una colección\n",
        "# y obtener listas de resultados\n",
        "def submit_queries_and_get_run(queries, collection, max_results=10):\n",
        "    # Inicializar el diccionario de ejecuciones (run)\n",
        "    run = {}\n",
        "\n",
        "    # Procesar cada consulta\n",
        "    for query in queries:\n",
        "        query_id = query[\"_id\"]\n",
        "        query_text = query[\"text\"].lower()\n",
        "\n",
        "        # Enviar la consulta a la colección y obtener los resultados\n",
        "        results = collection.query(\n",
        "            query_texts=[query_text],\n",
        "            n_results=max_results\n",
        "        )\n",
        "\n",
        "        # Almacenar los IDs de los resultados en el diccionario de ejecuciones\n",
        "        run[query_id] = results['ids'][0]\n",
        "\n",
        "    return run"
      ],
      "metadata": {
        "id": "0vLDYbn90LQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estas son las consultas originales en la colección de evaluación; fueron recopiladas por los autores de la\n",
        "colección, por lo que el desajuste de vocabulario entre las consultas y los documentos debería ser mínimo."
      ],
      "metadata": {
        "id": "wQicEr6I0dUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_run = submit_queries_and_get_run(queries, collection)"
      ],
      "metadata": {
        "id": "KYRGvb9u0e_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora definimos el codigo para calcular la precisión y la exhaustividad y F1-score"
      ],
      "metadata": {
        "id": "4b6MRh5n019K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para calcular precisión, recall y F1-score tanto en promedio micro como macro\n",
        "def compute_precision_recall_f1(run, relevance_judgements):\n",
        "    # Inicializar listas para almacenar los valores de precisión, recall y F1 para cada consulta\n",
        "    precision_values = []\n",
        "    recall_values = []\n",
        "    f1_values = []\n",
        "\n",
        "    # Inicializar conteos globales para el promedio micro\n",
        "    global_retrieved = 0\n",
        "    global_relevant = 0\n",
        "    global_retrieved_and_relevant = 0\n",
        "\n",
        "    # Calcular precisión, recall y F1-score para cada consulta\n",
        "    for query_id in run.keys():\n",
        "        retrieved_results = run[query_id]\n",
        "        relevant_results = relevance_judgements[query_id]\n",
        "        relevant_and_retrieved = set(retrieved_results) & set(relevant_results)\n",
        "\n",
        "        # Actualizar conteos globales\n",
        "        global_retrieved += len(retrieved_results)\n",
        "        global_relevant += len(relevant_results)\n",
        "        global_retrieved_and_relevant += len(relevant_and_retrieved)\n",
        "\n",
        "        # Calcular precisión y recall\n",
        "        precision = len(relevant_and_retrieved) / len(retrieved_results) if len(retrieved_results) > 0 else 0\n",
        "        recall = len(relevant_and_retrieved) / len(relevant_results) if len(relevant_results) > 0 else 0\n",
        "\n",
        "        # Calcular F1-score si ambos precisión y recall son mayores a cero\n",
        "        if (precision + recall) > 0:\n",
        "            f1 = 2 * (precision * recall) / (precision + recall)\n",
        "            f1_values.append(f1)\n",
        "\n",
        "        # Agregar precisión y recall de la consulta actual\n",
        "        precision_values.append(precision)\n",
        "        recall_values.append(recall)\n",
        "\n",
        "    # Calcular promedios macro\n",
        "    macro_average_precision = sum(precision_values) / len(precision_values) if precision_values else 0\n",
        "    macro_average_recall = sum(recall_values) / len(recall_values) if recall_values else 0\n",
        "    macro_average_f1 = sum(f1_values) / len(f1_values) if f1_values else 0\n",
        "\n",
        "    # Imprimir métricas promediadas macro\n",
        "    print(f\"Precisión promediada macro: {round(macro_average_precision,3)}\")\n",
        "    print(f\"Recall promediado macro: {round(macro_average_recall,3)}\")\n",
        "    print(f\"F1 promediado macro: {round(macro_average_f1,3)}\")\n",
        "    print(\"\")\n",
        "\n",
        "    # Calcular promedios micro\n",
        "    micro_average_precision = global_retrieved_and_relevant / global_retrieved if global_retrieved > 0 else 0\n",
        "    micro_average_recall = global_retrieved_and_relevant / global_relevant if global_relevant > 0 else 0\n",
        "    micro_average_f1 = (2 * (micro_average_precision * micro_average_recall) /\n",
        "                        (micro_average_precision + micro_average_recall)) if (micro_average_precision + micro_average_recall) > 0 else 0\n",
        "\n",
        "    # Imprimir métricas promediadas micro\n",
        "    print(f\"Precisión promediada micro: {round(micro_average_precision,3)}\")\n",
        "    print(f\"Recall promediado micro: {round(micro_average_recall,3)}\")\n",
        "    print(f\"F1 promediado micro: {round(micro_average_f1,3)}\")\n"
      ],
      "metadata": {
        "id": "RvgsPv_r028H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tambien tenemos que cargar los juicios, los cuales los cargaremos como hasta ahora, aunque sera necesaria una modificacion:"
      ],
      "metadata": {
        "id": "FD-10LTb1Dyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos los juicios de relevancia desde qrels.tsv\n",
        "relevance_judgements = {}\n",
        "with open(\"qrels.tsv\", \"r\", encoding=\"utf-8\") as f:\n",
        "    next(f)  # Saltar la cabecera\n",
        "    for line in f:\n",
        "        query_id, corpus_id, score = line.strip().split(\"\\t\")\n",
        "        if int(score) > 0:  # Considerar solo documentos relevantes (score > 0)\n",
        "            if query_id not in relevance_judgements:\n",
        "                relevance_judgements[query_id] = []\n",
        "            relevance_judgements[query_id].append(corpus_id)\n",
        "\n",
        "# Formatear los juicios de relevancia\n",
        "relevance_judgements_reformat = {}\n",
        "for query_id, rel_docs in relevance_judgements.items():\n",
        "    relevance_judgements_reformat[query_id] = rel_docs\n",
        "\n",
        "# Imprimir el resultado formateado\n",
        "print(relevance_judgements_reformat)\n"
      ],
      "metadata": {
        "id": "sNMSqcR91LDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con todo lo anterior ya podemos comprobar que tan bueno es la busqueda con este procedimiento:"
      ],
      "metadata": {
        "id": "dleLCoBG3tJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compute_precision_recall_f1(original_run, relevance_judgements_reformat)"
      ],
      "metadata": {
        "id": "PamG5JTL3Yg_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}