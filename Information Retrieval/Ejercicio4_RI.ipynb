{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHcbw/wFUv+Q2LeNefVXt5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DiegoLangreo7/EII-RI/blob/main/Ejercicio4_RI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ej. 4. Evaluación de la técnica de expansión de consultas**"
      ],
      "metadata": {
        "id": "3P04mw1aG0rM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es este nuevo ejercicio, se nos pide evaluar el sistema desarrollado en el tercer ejercicio generando de manera automática rondas de resultados al estilo del segundo ejercicio.\n",
        "\n",
        "### Para ello empezaremos descargando la coleccion, y importando las librerias a utilizar, ademas de dejar ya indexada la coleccion inicial:"
      ],
      "metadata": {
        "id": "b-tpjzIBBdOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalamos el paquete bm25s\n",
        "!pip install bm25s[full]\n",
        "\n",
        "# Importamos las librerías necesarias\n",
        "import bm25s\n",
        "import Stemmer\n",
        "import json\n",
        "from collections import Counter\n",
        "import math\n",
        "from heapq import heappush, heappop\n",
        "\n",
        "# Descargamos la colección y las consultas\n",
        "!gdown 19pzNFYIch8rj9d3kyq-171V8vRa_wLBC\n",
        "!unzip -o trec-covid-RI.zip\n",
        "\n",
        "# Parseamos la colección\n",
        "with open(\"corpus.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    corpus_content = [json.loads(line) for line in f]\n",
        "\n",
        "# Indexamos la colección\n",
        "corpus_verbatim = []\n",
        "corpus_plaintext = []\n",
        "for entry in corpus_content:\n",
        "    document = {\n",
        "        \"id\": entry[\"_id\"],\n",
        "        \"title\": entry[\"title\"].lower(),\n",
        "        \"text\": entry[\"text\"].lower()\n",
        "    }\n",
        "    corpus_verbatim.append(document)\n",
        "    corpus_plaintext.append(entry[\"text\"].lower())"
      ],
      "metadata": {
        "id": "Exn3et9RCPkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Una vez ya tenemos nuestra coleccion, utilizamos nuevamente la funcion proporcionada por el profesor de practicas para calcular las frecuencias de términos para nuestro corpus, usada en el ejercicio 3:"
      ],
      "metadata": {
        "id": "dicFL_DdINZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_term_frequencies_from_corpus_tokenized(corpus_tokenized):\n",
        "    from collections import Counter\n",
        "\n",
        "    tmp = dict()\n",
        "\n",
        "    for document in corpus_tokenized[0]:\n",
        "        freqs = dict(Counter(document))\n",
        "        for token, freq in freqs.items():\n",
        "            try:\n",
        "                tmp[token] += freq\n",
        "            except:\n",
        "                tmp[token] = freq\n",
        "\n",
        "    inverted_vocab = {corpus_tokenized[1][key]: key for key in corpus_tokenized[1].keys()}\n",
        "\n",
        "    total_freqs = dict()\n",
        "\n",
        "    for key, freq in dict(tmp).items():\n",
        "        term = inverted_vocab[key]\n",
        "        total_freqs[term] = freq\n",
        "\n",
        "    return total_freqs"
      ],
      "metadata": {
        "id": "fWAKmv8kIXyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con esto ya podemos sacar nuevamente los terms de la coleccion inicial:"
      ],
      "metadata": {
        "id": "XT2Sp9vJIieB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_corpus_tokenized = bm25s.tokenize(corpus_plaintext, stopwords=None, stemmer=Stemmer.Stemmer(\"english\"))\n",
        "initial_compute_term_frequencies = compute_term_frequencies_from_corpus_tokenized(initial_corpus_tokenized)\n",
        "print(initial_compute_term_frequencies)"
      ],
      "metadata": {
        "id": "N8CLh7tvIlfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ahora volvemos a declarar la funcion traida de la implementacion java, junto a nuestra funcion personal para expandir consultas\n",
        "\n"
      ],
      "metadata": {
        "id": "DUANxJWqIoL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogLikelihood:\n",
        "    @staticmethod\n",
        "    def entropy(*elements):\n",
        "        total_sum = sum(elements)\n",
        "        result = sum(LogLikelihood.x_log_x(e) for e in elements)\n",
        "        return LogLikelihood.x_log_x(total_sum) - result\n",
        "\n",
        "    @staticmethod\n",
        "    def x_log_x(x):\n",
        "        return 0.0 if x == 0 else x * math.log(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def log_likelihood_ratio(k11, k12, k21, k22):\n",
        "        if any(k < 0 for k in [k11, k12, k21, k22]):\n",
        "            raise ValueError(\"Counts must be non-negative.\")\n",
        "\n",
        "        row_entropy = LogLikelihood.entropy(k11 + k12, k21 + k22)\n",
        "        column_entropy = LogLikelihood.entropy(k11 + k21, k12 + k22)\n",
        "        matrix_entropy = LogLikelihood.entropy(k11, k12, k21, k22)\n",
        "\n",
        "        if row_entropy + column_entropy < matrix_entropy:\n",
        "            return 0.0\n",
        "\n",
        "        return 2.0 * (row_entropy + column_entropy - matrix_entropy)\n",
        "\n",
        "    @staticmethod\n",
        "    def root_log_likelihood_ratio(k11, k12, k21, k22):\n",
        "        llr = LogLikelihood.log_likelihood_ratio(k11, k12, k21, k22)\n",
        "        sqrt_llr = math.sqrt(llr)\n",
        "        if k11 / (k11 + k12) < k21 / (k21 + k22):\n",
        "            sqrt_llr = -sqrt_llr\n",
        "        return sqrt_llr\n",
        "\n",
        "    @staticmethod\n",
        "    def compare_frequencies(a, b, max_return, threshold):\n",
        "        total_a = sum(a.values())\n",
        "        total_b = sum(b.values())\n",
        "\n",
        "        best = []\n",
        "\n",
        "        for item in a:\n",
        "            LogLikelihood._compare_and_add(a, b, max_return, threshold, total_a, total_b, best, item)\n",
        "\n",
        "        if threshold < 0:\n",
        "            for item in b:\n",
        "                if item not in a:\n",
        "                    LogLikelihood._compare_and_add(a, b, max_return, threshold, total_a, total_b, best, item)\n",
        "\n",
        "        return sorted(best, key=lambda x: -x.score)\n",
        "\n",
        "    @staticmethod\n",
        "    def _compare_and_add(a, b, max_return, threshold, total_a, total_b, best, item):\n",
        "        k_a = a.get(item, 0)\n",
        "        k_b = b.get(item, 0)\n",
        "        score = LogLikelihood.root_log_likelihood_ratio(k_a, total_a - k_a, k_b, total_b - k_b)\n",
        "\n",
        "        if score >= threshold:\n",
        "            heappush(best, ScoredItem(item, score))\n",
        "            if len(best) > max_return:\n",
        "                heappop(best)\n",
        "\n",
        "class ScoredItem:\n",
        "    def __init__(self, item, score):\n",
        "        self.item = item\n",
        "        self.score = score\n",
        "\n",
        "    def __lt__(self, other):\n",
        "        return self.score < other.score"
      ],
      "metadata": {
        "id": "DPHLseIzIvUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Devuelve el texto plano de los n documentos mas relevantes\n",
        "def get_n_valuable_documents_for_query(query, retriever, n):\n",
        "    # Convierte el texto de la consulta a minúsculas\n",
        "    query_string = query[\"text\"].lower()\n",
        "\n",
        "    # Tokeniza la consulta\n",
        "    query_tokenized = bm25s.tokenize(\n",
        "        query_string,\n",
        "        stopwords=None,\n",
        "        stemmer=Stemmer.Stemmer(\"english\"),\n",
        "        show_progress=False\n",
        "    )\n",
        "\n",
        "    # Recupera los documentos relevantes usando el índice BM25\n",
        "    results = retriever.retrieve(\n",
        "        query_tokenized,\n",
        "        corpus=retriever.corpus,\n",
        "        k=n,\n",
        "        return_as=\"tuple\",\n",
        "        show_progress=False\n",
        "    )\n",
        "\n",
        "    # Itera sobre los documentos recuperados y agrega su texto a text_content\n",
        "    text_content = \"\"\n",
        "    for document in results.documents[0]:\n",
        "        text_content += document['text'] + \" \"\n",
        "\n",
        "    return text_content\n",
        "\n",
        "\n",
        "# Devuelve los terminos con mas frecuencia para un texto plano (el de la query)\n",
        "def get_term_frequency_for_query(query_text):\n",
        "    queried_corpus_tokenized = bm25s.tokenize(query_text, stopwords=None, stemmer=Stemmer.Stemmer(\"english\"))\n",
        "    return compute_term_frequencies_from_corpus_tokenized(queried_corpus_tokenized)\n",
        "\n",
        "\n",
        "# Compara las frecuencias de los terminos de toda la coleccion, con la los archivos de la query\n",
        "def comparate_frequencies(dictA, dictB, m):\n",
        "    result = LogLikelihood.compare_frequencies(dictA, dictB, max_return=m, threshold=0)\n",
        "    return result\n",
        "\n",
        "\n",
        "# Expande la query para implementar los terminos resutantes del algoritmo explicado\n",
        "def query_expansion(query, retriever, n, m):\n",
        "    text = get_n_valuable_documents_for_query(query, retriever, n)\n",
        "    term_frequencies = get_term_frequency_for_query(text)\n",
        "    tf_terms = comparate_frequencies(initial_compute_term_frequencies, term_frequencies, m)\n",
        "\n",
        "    # Accede al atributo item de cada ScoredItem\n",
        "    tf_terms_items = [item.item for item in tf_terms]\n",
        "\n",
        "    original_terms = query[\"text\"].split()\n",
        "\n",
        "    # Extiende la consulta con los términos relevantes\n",
        "    the_extended_one = original_terms[:]\n",
        "    for term in tf_terms_items:\n",
        "        if term not in the_extended_one:\n",
        "            the_extended_one.append(term)\n",
        "\n",
        "    return the_extended_one\n",
        "\n",
        "# Expande la lista de queries para unos parametros dados\n",
        "def get_expanded_queries(queries, retriever, n, m):\n",
        "    expanded_queries = []\n",
        "    for query in queries:\n",
        "        expanded_query = query_expansion(query, retriever, n, m)\n",
        "        expanded_queries.append({\n",
        "            \"_id\": query[\"_id\"],\n",
        "            \"text\": \" \".join(expanded_query)\n",
        "        })\n",
        "    return expanded_queries"
      ],
      "metadata": {
        "id": "iLROvEXlI4hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ahora declaramos la funcion para obtener la lista de resultados, junto a la de calcular las metricas:"
      ],
      "metadata": {
        "id": "FilEXcFXJF9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def submit_queries_and_get_run(queries, retriever, max_results=100):\n",
        "    # Inicializa un diccionario para almacenar los resultados de cada consulta\n",
        "    run = {}\n",
        "\n",
        "    # Itera sobre cada consulta en la lista de consultas\n",
        "    for query in queries:\n",
        "        # Obtiene el ID único de la consulta\n",
        "        query_id = query[\"_id\"]\n",
        "\n",
        "        # Convierte el texto de la consulta a minúsculas\n",
        "        query_string = query[\"text\"].lower()\n",
        "\n",
        "        # Tokeniza la consulta utilizando las stopwords y el stemmer proporcionados\n",
        "        query_tokenized = bm25s.tokenize(\n",
        "            query_string,\n",
        "            stopwords=None,\n",
        "            stemmer=Stemmer.Stemmer(\"english\"),\n",
        "            show_progress=False\n",
        "        )\n",
        "\n",
        "        # Recupera los documentos relevantes usando el índice BM25\n",
        "        results = retriever.retrieve(\n",
        "            query_tokenized,\n",
        "            corpus=retriever.corpus,\n",
        "            k=max_results,\n",
        "            return_as=\"tuple\",\n",
        "            show_progress=False\n",
        "        )\n",
        "\n",
        "        # Obtiene los documentos recuperados y sus scores de relevancia\n",
        "        returned_documents = results.documents[0]\n",
        "\n",
        "        # Inicializa una lista para almacenar los IDs de los documentos recuperados\n",
        "        returned_ids = []\n",
        "        for i in range(len(returned_documents)):\n",
        "            # Agrega el ID de cada documento recuperado a la lista\n",
        "            returned_ids.append(str(returned_documents[i][\"id\"]))\n",
        "\n",
        "        # Asocia la lista de IDs recuperados con el ID de la consulta en el diccionario\n",
        "        run[query_id] = returned_ids\n",
        "\n",
        "    # Devuelve el diccionario con los resultados de todas las consultas\n",
        "    return run"
      ],
      "metadata": {
        "id": "LP_axactJRxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función adaptada para calcular precision, recall y F1\n",
        "\n",
        "def compute_precision_recall_f1(run, relevance_judgements):\n",
        "    precision_values = []\n",
        "    recall_values = []\n",
        "    f1_values = []\n",
        "\n",
        "    global_retrieved = 0\n",
        "    global_relevant = 0\n",
        "    global_retrieved_and_relevant = 0\n",
        "\n",
        "    for query_id in run.keys():\n",
        "        retrieved_results = run[query_id]\n",
        "        relevant_results = relevance_judgements.get(query_id, [])\n",
        "        relevant_and_retrieved = set(retrieved_results) & set(relevant_results)\n",
        "\n",
        "        global_retrieved += len(retrieved_results)\n",
        "        global_relevant += len(relevant_results)\n",
        "        global_retrieved_and_relevant += len(relevant_and_retrieved)\n",
        "\n",
        "        precision = len(relevant_and_retrieved) / len(retrieved_results) if len(retrieved_results) > 0 else 0\n",
        "        recall = len(relevant_and_retrieved) / len(relevant_results) if len(relevant_results) > 0 else 0\n",
        "\n",
        "        if (precision + recall) > 0:\n",
        "            f1 = 2 * (precision * recall) / (precision + recall)\n",
        "            f1_values.append(f1)\n",
        "\n",
        "        precision_values.append(precision)\n",
        "        recall_values.append(recall)\n",
        "\n",
        "    macro_average_precision = sum(precision_values) / len(precision_values) if precision_values else 0\n",
        "    macro_average_recall = sum(recall_values) / len(recall_values) if recall_values else 0\n",
        "    macro_average_f1 = sum(f1_values) / len(f1_values) if f1_values else 0\n",
        "\n",
        "    micro_average_precision = global_retrieved_and_relevant / global_retrieved if global_retrieved > 0 else 0\n",
        "    micro_average_recall = global_retrieved_and_relevant / global_relevant if global_relevant > 0 else 0\n",
        "    micro_average_f1 = (2 * (micro_average_precision * micro_average_recall) /\n",
        "                        (micro_average_precision + micro_average_recall)) if (micro_average_precision + micro_average_recall) > 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"macro_precision\": macro_average_precision,\n",
        "        \"macro_recall\": macro_average_recall,\n",
        "        \"macro_f1\": macro_average_f1,\n",
        "        \"micro_precision\": micro_average_precision,\n",
        "        \"micro_recall\": micro_average_recall,\n",
        "        \"micro_f1\": micro_average_f1\n",
        "    }\n"
      ],
      "metadata": {
        "id": "AMhkJSMgJVwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ahora empezamos a inicializar los objetos que usaremos:"
      ],
      "metadata": {
        "id": "dtK5FIPcKbhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Queries cargadas\n",
        "queries = []\n",
        "\n",
        "with open(\"queries.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        queries.append(json.loads(line))\n",
        "\n",
        "# Flavour cargado\n",
        "bm25_flavor = \"robertson\"\n",
        "idf_flavor = \"robertson\"\n",
        "\n",
        "# Retriever generado\n",
        "retriever = bm25s.BM25(corpus=corpus_verbatim,method=bm25_flavor,idf_method=idf_flavor)\n",
        "retriever.index(initial_corpus_tokenized, show_progress=True)\n",
        "\n",
        "# Cargamos los juicios de relevancia desde qrels.tsv\n",
        "relevance_judgements = {}\n",
        "with open(\"qrels.tsv\", \"r\", encoding=\"utf-8\") as f:\n",
        "    next(f)  # Saltar la cabecera\n",
        "    for line in f:\n",
        "        query_id, corpus_id, score = line.strip().split(\"\\t\")\n",
        "        if int(score) > 0:  # Considerar solo documentos relevantes (score > 0)\n",
        "            if query_id not in relevance_judgements:\n",
        "                relevance_judgements[query_id] = []\n",
        "            relevance_judgements[query_id].append(corpus_id)"
      ],
      "metadata": {
        "id": "70YkFPh2KlKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Y empezamos la ejecucion:"
      ],
      "metadata": {
        "id": "d7W5Y3wzLAia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for n in range(1, 6):     #1-5\n",
        "    for m in range(3, 6):       #1-6\n",
        "        print(f\"Configuraciom: n={n}, m={m}\")\n",
        "        expanded_queries = get_expanded_queries(queries, retriever, n, m)\n",
        "        run_original = submit_queries_and_get_run(expanded_queries, retriever)\n",
        "\n",
        "        metrics = compute_precision_recall_f1(run_original, relevance_judgements)\n",
        "        print(f\"Macro-averaged Precision: {metrics['macro_precision']:.3f}\")\n",
        "        print(f\"Macro-averaged Recall: {metrics['macro_recall']:.3f}\")\n",
        "        print(f\"Macro-averaged F1: {metrics['macro_f1']:.3f}\\n\")\n",
        "        print(f\"Micro-averaged Precision: {metrics['micro_precision']:.3f}\")\n",
        "        print(f\"Micro-averaged Recall: {metrics['micro_recall']:.3f}\")\n",
        "        print(f\"Micro-averaged F1: {metrics['micro_f1']:.3f}\\n\")\n"
      ],
      "metadata": {
        "id": "N5WrQAkhLCQh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}